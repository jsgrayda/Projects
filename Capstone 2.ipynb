{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1\n",
    "Go out and find a dataset of interest. It could be from one of our recommended resources, some other aggregation, or scraped yourself. Just make sure it has lots of variables in it, including an outcome of interest to you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset of Interest: Online News Popularity from UCI\n",
    "link: https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity\n",
    "link to what the variables mean: https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>timedelta</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>n_non_stop_words</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>...</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>shares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://mashable.com/2013/01/07/amazon-instant-...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>0.663594</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.815385</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.350000</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://mashable.com/2013/01/07/ap-samsung-spon...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>0.604743</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.791946</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.118750</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://mashable.com/2013/01/07/apple-40-billio...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>0.575130</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.466667</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://mashable.com/2013/01/07/astronaut-notre...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>531.0</td>\n",
       "      <td>0.503788</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.665635</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-0.369697</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://mashable.com/2013/01/07/att-u-verse-apps/</td>\n",
       "      <td>731.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1072.0</td>\n",
       "      <td>0.415646</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.540890</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.220192</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url   timedelta  \\\n",
       "0  http://mashable.com/2013/01/07/amazon-instant-...       731.0   \n",
       "1  http://mashable.com/2013/01/07/ap-samsung-spon...       731.0   \n",
       "2  http://mashable.com/2013/01/07/apple-40-billio...       731.0   \n",
       "3  http://mashable.com/2013/01/07/astronaut-notre...       731.0   \n",
       "4   http://mashable.com/2013/01/07/att-u-verse-apps/       731.0   \n",
       "\n",
       "    n_tokens_title   n_tokens_content   n_unique_tokens   n_non_stop_words  \\\n",
       "0             12.0              219.0          0.663594                1.0   \n",
       "1              9.0              255.0          0.604743                1.0   \n",
       "2              9.0              211.0          0.575130                1.0   \n",
       "3              9.0              531.0          0.503788                1.0   \n",
       "4             13.0             1072.0          0.415646                1.0   \n",
       "\n",
       "    n_non_stop_unique_tokens   num_hrefs   num_self_hrefs   num_imgs  ...  \\\n",
       "0                   0.815385         4.0              2.0        1.0  ...   \n",
       "1                   0.791946         3.0              1.0        1.0  ...   \n",
       "2                   0.663866         3.0              1.0        1.0  ...   \n",
       "3                   0.665635         9.0              0.0        1.0  ...   \n",
       "4                   0.540890        19.0             19.0       20.0  ...   \n",
       "\n",
       "    min_positive_polarity   max_positive_polarity   avg_negative_polarity  \\\n",
       "0                0.100000                     0.7               -0.350000   \n",
       "1                0.033333                     0.7               -0.118750   \n",
       "2                0.100000                     1.0               -0.466667   \n",
       "3                0.136364                     0.8               -0.369697   \n",
       "4                0.033333                     1.0               -0.220192   \n",
       "\n",
       "    min_negative_polarity   max_negative_polarity   title_subjectivity  \\\n",
       "0                  -0.600               -0.200000             0.500000   \n",
       "1                  -0.125               -0.100000             0.000000   \n",
       "2                  -0.800               -0.133333             0.000000   \n",
       "3                  -0.600               -0.166667             0.000000   \n",
       "4                  -0.500               -0.050000             0.454545   \n",
       "\n",
       "    title_sentiment_polarity   abs_title_subjectivity  \\\n",
       "0                  -0.187500                 0.000000   \n",
       "1                   0.000000                 0.500000   \n",
       "2                   0.000000                 0.500000   \n",
       "3                   0.000000                 0.500000   \n",
       "4                   0.136364                 0.045455   \n",
       "\n",
       "    abs_title_sentiment_polarity   shares  \n",
       "0                       0.187500      593  \n",
       "1                       0.000000      711  \n",
       "2                       0.000000     1500  \n",
       "3                       0.000000     1200  \n",
       "4                       0.136364      505  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import data\n",
    "import pandas as pd \n",
    "df_OG = pd.read_csv(\"capstone 2.csv\") \n",
    "df_OG.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2\n",
    "Explore the data. Get to know the data. Spend a lot of time going over its quirks and peccadilloes. You should understand how it was gathered, what's in it, and what the variables look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['url', ' timedelta', ' n_tokens_title', ' n_tokens_content',\n",
       "       ' n_unique_tokens', ' n_non_stop_words', ' n_non_stop_unique_tokens',\n",
       "       ' num_hrefs', ' num_self_hrefs', ' num_imgs', ' num_videos',\n",
       "       ' average_token_length', ' num_keywords', ' data_channel_is_lifestyle',\n",
       "       ' data_channel_is_entertainment', ' data_channel_is_bus',\n",
       "       ' data_channel_is_socmed', ' data_channel_is_tech',\n",
       "       ' data_channel_is_world', ' kw_min_min', ' kw_max_min', ' kw_avg_min',\n",
       "       ' kw_min_max', ' kw_max_max', ' kw_avg_max', ' kw_min_avg',\n",
       "       ' kw_max_avg', ' kw_avg_avg', ' self_reference_min_shares',\n",
       "       ' self_reference_max_shares', ' self_reference_avg_sharess',\n",
       "       ' weekday_is_monday', ' weekday_is_tuesday', ' weekday_is_wednesday',\n",
       "       ' weekday_is_thursday', ' weekday_is_friday', ' weekday_is_saturday',\n",
       "       ' weekday_is_sunday', ' is_weekend', ' LDA_00', ' LDA_01', ' LDA_02',\n",
       "       ' LDA_03', ' LDA_04', ' global_subjectivity',\n",
       "       ' global_sentiment_polarity', ' global_rate_positive_words',\n",
       "       ' global_rate_negative_words', ' rate_positive_words',\n",
       "       ' rate_negative_words', ' avg_positive_polarity',\n",
       "       ' min_positive_polarity', ' max_positive_polarity',\n",
       "       ' avg_negative_polarity', ' min_negative_polarity',\n",
       "       ' max_negative_polarity', ' title_subjectivity',\n",
       "       ' title_sentiment_polarity', ' abs_title_subjectivity',\n",
       "       ' abs_title_sentiment_polarity', ' shares'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_OG.columns\n",
    "# i want to drop df['url']\n",
    "# then delete the spaces before each column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_OG['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([' timedelta', ' n_tokens_title', ' n_tokens_content',\n",
       "       ' n_unique_tokens', ' n_non_stop_words', ' n_non_stop_unique_tokens',\n",
       "       ' num_hrefs', ' num_self_hrefs', ' num_imgs', ' num_videos',\n",
       "       ' average_token_length', ' num_keywords', ' data_channel_is_lifestyle',\n",
       "       ' data_channel_is_entertainment', ' data_channel_is_bus',\n",
       "       ' data_channel_is_socmed', ' data_channel_is_tech',\n",
       "       ' data_channel_is_world', ' kw_min_min', ' kw_max_min', ' kw_avg_min',\n",
       "       ' kw_min_max', ' kw_max_max', ' kw_avg_max', ' kw_min_avg',\n",
       "       ' kw_max_avg', ' kw_avg_avg', ' self_reference_min_shares',\n",
       "       ' self_reference_max_shares', ' self_reference_avg_sharess',\n",
       "       ' weekday_is_monday', ' weekday_is_tuesday', ' weekday_is_wednesday',\n",
       "       ' weekday_is_thursday', ' weekday_is_friday', ' weekday_is_saturday',\n",
       "       ' weekday_is_sunday', ' is_weekend', ' LDA_00', ' LDA_01', ' LDA_02',\n",
       "       ' LDA_03', ' LDA_04', ' global_subjectivity',\n",
       "       ' global_sentiment_polarity', ' global_rate_positive_words',\n",
       "       ' global_rate_negative_words', ' rate_positive_words',\n",
       "       ' rate_negative_words', ' avg_positive_polarity',\n",
       "       ' min_positive_polarity', ' max_positive_polarity',\n",
       "       ' avg_negative_polarity', ' min_negative_polarity',\n",
       "       ' max_negative_polarity', ' title_subjectivity',\n",
       "       ' title_sentiment_polarity', ' abs_title_subjectivity',\n",
       "       ' abs_title_sentiment_polarity', ' shares'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_OG.columns # code above worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_OG.columns = df_OG.columns.str.lstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_alt = df_OG.copy()\n",
    "#make a copy of the OG df, df_alt is where I'll add the other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timedelta</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>n_non_stop_words</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>num_videos</th>\n",
       "      <th>...</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>shares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>731.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>0.663594</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.815385</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.350000</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>0.604743</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.791946</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.118750</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>0.575130</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.466667</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>531.0</td>\n",
       "      <td>0.503788</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.665635</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-0.369697</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>731.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1072.0</td>\n",
       "      <td>0.415646</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.540890</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.220192</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   timedelta  n_tokens_title  n_tokens_content  n_unique_tokens  \\\n",
       "0      731.0            12.0             219.0         0.663594   \n",
       "1      731.0             9.0             255.0         0.604743   \n",
       "2      731.0             9.0             211.0         0.575130   \n",
       "3      731.0             9.0             531.0         0.503788   \n",
       "4      731.0            13.0            1072.0         0.415646   \n",
       "\n",
       "   n_non_stop_words  n_non_stop_unique_tokens  num_hrefs  num_self_hrefs  \\\n",
       "0               1.0                  0.815385        4.0             2.0   \n",
       "1               1.0                  0.791946        3.0             1.0   \n",
       "2               1.0                  0.663866        3.0             1.0   \n",
       "3               1.0                  0.665635        9.0             0.0   \n",
       "4               1.0                  0.540890       19.0            19.0   \n",
       "\n",
       "   num_imgs  num_videos  ...  min_positive_polarity  max_positive_polarity  \\\n",
       "0       1.0         0.0  ...               0.100000                    0.7   \n",
       "1       1.0         0.0  ...               0.033333                    0.7   \n",
       "2       1.0         0.0  ...               0.100000                    1.0   \n",
       "3       1.0         0.0  ...               0.136364                    0.8   \n",
       "4      20.0         0.0  ...               0.033333                    1.0   \n",
       "\n",
       "   avg_negative_polarity  min_negative_polarity  max_negative_polarity  \\\n",
       "0              -0.350000                 -0.600              -0.200000   \n",
       "1              -0.118750                 -0.125              -0.100000   \n",
       "2              -0.466667                 -0.800              -0.133333   \n",
       "3              -0.369697                 -0.600              -0.166667   \n",
       "4              -0.220192                 -0.500              -0.050000   \n",
       "\n",
       "   title_subjectivity  title_sentiment_polarity  abs_title_subjectivity  \\\n",
       "0            0.500000                 -0.187500                0.000000   \n",
       "1            0.000000                  0.000000                0.500000   \n",
       "2            0.000000                  0.000000                0.500000   \n",
       "3            0.000000                  0.000000                0.500000   \n",
       "4            0.454545                  0.136364                0.045455   \n",
       "\n",
       "   abs_title_sentiment_polarity  shares  \n",
       "0                      0.187500     593  \n",
       "1                      0.000000     711  \n",
       "2                      0.000000    1500  \n",
       "3                      0.000000    1200  \n",
       "4                      0.136364     505  \n",
       "\n",
       "[5 rows x 60 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_alt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'll be adding these variables to see if they will have better correlation to shares\n",
    "df_alt['readability'] = df_alt['num_imgs'] + df_alt['num_videos']\n",
    "df_alt['readability_2'] = df_alt['readability'] - df_alt['n_non_stop_words']\n",
    "df_alt['article_is_informative'] = df_alt['data_channel_is_bus'] + df_alt['data_channel_is_world']\n",
    "df_alt['article_is_leisure'] = df_alt['data_channel_is_lifestyle'] + df_alt['data_channel_is_entertainment'] + df_alt['data_channel_is_socmed']\n",
    "\n",
    "#second time\n",
    "df_alt['readable_informative'] = df_alt['article_is_informative'] + df_alt['readability_2']\n",
    "df_alt['readable_leisure'] =  df_alt['article_is_leisure'] + df_alt['readability_2']\n",
    "\n",
    "#third time\n",
    "df_alt['article_is_boring'] = (df_alt['article_is_informative'] + df_alt['n_tokens_title'] + df_alt['n_non_stop_words']) - df_alt['readability']\n",
    "\n",
    "#fourth time\n",
    "df_alt['pub_on_weekday'] = df_alt['weekday_is_monday'] + df_alt['weekday_is_tuesday'] + df_alt['weekday_is_wednesday'] + df_alt['weekday_is_thursday'] + df_alt['weekday_is_friday']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['timedelta', 'n_tokens_title', 'n_tokens_content', 'n_unique_tokens',\n",
       "       'n_non_stop_words', 'n_non_stop_unique_tokens', 'num_hrefs',\n",
       "       'num_self_hrefs', 'num_imgs', 'num_videos', 'average_token_length',\n",
       "       'num_keywords', 'data_channel_is_lifestyle',\n",
       "       'data_channel_is_entertainment', 'data_channel_is_bus',\n",
       "       'data_channel_is_socmed', 'data_channel_is_tech',\n",
       "       'data_channel_is_world', 'kw_min_min', 'kw_max_min', 'kw_avg_min',\n",
       "       'kw_min_max', 'kw_max_max', 'kw_avg_max', 'kw_min_avg', 'kw_max_avg',\n",
       "       'kw_avg_avg', 'self_reference_min_shares', 'self_reference_max_shares',\n",
       "       'self_reference_avg_sharess', 'weekday_is_monday', 'weekday_is_tuesday',\n",
       "       'weekday_is_wednesday', 'weekday_is_thursday', 'weekday_is_friday',\n",
       "       'weekday_is_saturday', 'weekday_is_sunday', 'is_weekend', 'LDA_00',\n",
       "       'LDA_01', 'LDA_02', 'LDA_03', 'LDA_04', 'global_subjectivity',\n",
       "       'global_sentiment_polarity', 'global_rate_positive_words',\n",
       "       'global_rate_negative_words', 'rate_positive_words',\n",
       "       'rate_negative_words', 'avg_positive_polarity', 'min_positive_polarity',\n",
       "       'max_positive_polarity', 'avg_negative_polarity',\n",
       "       'min_negative_polarity', 'max_negative_polarity', 'title_subjectivity',\n",
       "       'title_sentiment_polarity', 'abs_title_subjectivity',\n",
       "       'abs_title_sentiment_polarity', 'shares', 'readability',\n",
       "       'readability_2', 'article_is_informative', 'article_is_leisure',\n",
       "       'readable_informative', 'readable_leisure', 'article_is_boring',\n",
       "       'pub_on_weekday'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_alt.columns # deleting the space before each column name worked!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y is the target variable\n",
    "Y_OG = df_OG['shares']\n",
    "\n",
    "X_OG = df_OG[['timedelta', 'n_tokens_title', 'n_tokens_content', 'n_unique_tokens',\n",
    "       'n_non_stop_words', 'n_non_stop_unique_tokens', 'num_hrefs',\n",
    "       'num_self_hrefs', 'num_imgs', 'num_videos', 'average_token_length',\n",
    "       'num_keywords', 'data_channel_is_lifestyle',\n",
    "       'data_channel_is_entertainment', 'data_channel_is_bus',\n",
    "       'data_channel_is_socmed', 'data_channel_is_tech',\n",
    "       'data_channel_is_world', 'kw_min_min', 'kw_max_min', 'kw_avg_min',\n",
    "       'kw_min_max', 'kw_max_max', 'kw_avg_max', 'kw_min_avg', 'kw_max_avg',\n",
    "       'kw_avg_avg', 'self_reference_min_shares', 'self_reference_max_shares',\n",
    "       'self_reference_avg_sharess', 'weekday_is_monday', 'weekday_is_tuesday',\n",
    "       'weekday_is_wednesday', 'weekday_is_thursday', 'weekday_is_friday',\n",
    "       'weekday_is_saturday', 'weekday_is_sunday', 'is_weekend', 'LDA_00',\n",
    "       'LDA_01', 'LDA_02', 'LDA_03', 'LDA_04', 'global_subjectivity',\n",
    "       'global_sentiment_polarity', 'global_rate_positive_words',\n",
    "       'global_rate_negative_words', 'rate_positive_words',\n",
    "       'rate_negative_words', 'avg_positive_polarity', 'min_positive_polarity',\n",
    "       'max_positive_polarity', 'avg_negative_polarity',\n",
    "       'min_negative_polarity', 'max_negative_polarity', 'title_subjectivity',\n",
    "       'title_sentiment_polarity', 'abs_title_subjectivity',\n",
    "       'abs_title_sentiment_polarity']]\n",
    "\n",
    "# Import programs\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "#scaler = StandardScaler()\n",
    "#X_OG_std = scaler.fit_transform(X_OG)\n",
    "#Y_std = scaler.fit_transform(Y)\n",
    "\n",
    "\n",
    "X_OG_train, X_OG_test, y_OG_train, y_OG_test = train_test_split(X_OG, Y_OG, test_size = 0.2, random_state = 465)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y is the target variable\n",
    "Y_alt = df_alt['shares']\n",
    "\n",
    "X_alt = df_alt[['timedelta', 'n_tokens_title', 'n_tokens_content', 'n_unique_tokens',\n",
    "       'n_non_stop_words', 'n_non_stop_unique_tokens', 'num_hrefs',\n",
    "       'num_self_hrefs', 'num_imgs', 'num_videos', 'average_token_length',\n",
    "       'num_keywords', 'data_channel_is_lifestyle',\n",
    "       'data_channel_is_entertainment', 'data_channel_is_bus',\n",
    "       'data_channel_is_socmed', 'data_channel_is_tech',\n",
    "       'data_channel_is_world', 'kw_min_min', 'kw_max_min', 'kw_avg_min',\n",
    "       'kw_min_max', 'kw_max_max', 'kw_avg_max', 'kw_min_avg', 'kw_max_avg',\n",
    "       'kw_avg_avg', 'self_reference_min_shares', 'self_reference_max_shares',\n",
    "       'self_reference_avg_sharess', 'weekday_is_monday', 'weekday_is_tuesday',\n",
    "       'weekday_is_wednesday', 'weekday_is_thursday', 'weekday_is_friday',\n",
    "       'weekday_is_saturday', 'weekday_is_sunday', 'is_weekend', 'LDA_00',\n",
    "       'LDA_01', 'LDA_02', 'LDA_03', 'LDA_04', 'global_subjectivity',\n",
    "       'global_sentiment_polarity', 'global_rate_positive_words',\n",
    "       'global_rate_negative_words', 'rate_positive_words',\n",
    "       'rate_negative_words', 'avg_positive_polarity', 'min_positive_polarity',\n",
    "       'max_positive_polarity', 'avg_negative_polarity',\n",
    "       'min_negative_polarity', 'max_negative_polarity', 'title_subjectivity',\n",
    "       'title_sentiment_polarity', 'abs_title_subjectivity',\n",
    "       'abs_title_sentiment_polarity', 'readability',\n",
    "       'readability_2', 'article_is_informative', 'article_is_leisure',\n",
    "       'readable_informative', 'readable_leisure', 'article_is_boring',\n",
    "       'pub_on_weekday']]\n",
    "\n",
    "# Import programs\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "#scaler = StandardScaler()\n",
    "#X_alt_std = scaler.fit_transform(X_alt)\n",
    "#Y_std = scaler.fit_transform(Y)\n",
    "\n",
    "\n",
    "X_alt_train, X_alt_test, y_alt_train, y_alt_test = train_test_split(X_alt, Y_alt, test_size = 0.2, random_state = 465)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outcome of Interest: Predict number of shares \n",
    "### I want to make a Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['timedelta', 'n_tokens_title', 'n_tokens_content', 'n_unique_tokens',\n",
       "       'n_non_stop_words', 'n_non_stop_unique_tokens', 'num_hrefs',\n",
       "       'num_self_hrefs', 'num_imgs', 'num_videos', 'average_token_length',\n",
       "       'num_keywords', 'data_channel_is_lifestyle',\n",
       "       'data_channel_is_entertainment', 'data_channel_is_bus',\n",
       "       'data_channel_is_socmed', 'data_channel_is_tech',\n",
       "       'data_channel_is_world', 'kw_min_min', 'kw_max_min', 'kw_avg_min',\n",
       "       'kw_min_max', 'kw_max_max', 'kw_avg_max', 'kw_min_avg', 'kw_max_avg',\n",
       "       'kw_avg_avg', 'self_reference_min_shares', 'self_reference_max_shares',\n",
       "       'self_reference_avg_sharess', 'weekday_is_monday', 'weekday_is_tuesday',\n",
       "       'weekday_is_wednesday', 'weekday_is_thursday', 'weekday_is_friday',\n",
       "       'weekday_is_saturday', 'weekday_is_sunday', 'is_weekend', 'LDA_00',\n",
       "       'LDA_01', 'LDA_02', 'LDA_03', 'LDA_04', 'global_subjectivity',\n",
       "       'global_sentiment_polarity', 'global_rate_positive_words',\n",
       "       'global_rate_negative_words', 'rate_positive_words',\n",
       "       'rate_negative_words', 'avg_positive_polarity', 'min_positive_polarity',\n",
       "       'max_positive_polarity', 'avg_negative_polarity',\n",
       "       'min_negative_polarity', 'max_negative_polarity', 'title_subjectivity',\n",
       "       'title_sentiment_polarity', 'abs_title_subjectivity',\n",
       "       'abs_title_sentiment_polarity', 'shares', 'readability',\n",
       "       'readability_2', 'article_is_informative', 'article_is_leisure',\n",
       "       'readable_informative', 'readable_leisure', 'article_is_boring',\n",
       "       'pub_on_weekday'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_alt_corr = df_alt.corr()\n",
    "#df_corr['shares']\n",
    "df_alt_corr.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LDA_02                          -0.059163\n",
       "article_is_informative          -0.051278\n",
       "data_channel_is_world           -0.049497\n",
       "article_is_boring               -0.040514\n",
       "avg_negative_polarity           -0.032029\n",
       "average_token_length            -0.022007\n",
       "max_negative_polarity           -0.019300\n",
       "min_negative_polarity           -0.019297\n",
       "data_channel_is_entertainment   -0.017006\n",
       "pub_on_weekday                  -0.016958\n",
       "LDA_04                          -0.016622\n",
       "data_channel_is_tech            -0.013253\n",
       "rate_positive_words             -0.013241\n",
       "data_channel_is_bus             -0.012376\n",
       "LDA_01                          -0.010183\n",
       "article_is_leisure              -0.008864\n",
       "weekday_is_thursday             -0.008833\n",
       "weekday_is_tuesday              -0.007941\n",
       "rate_negative_words             -0.005183\n",
       "weekday_is_friday               -0.003884\n",
       "weekday_is_wednesday            -0.003801\n",
       "LDA_00                          -0.003793\n",
       "num_self_hrefs                  -0.001900\n",
       "kw_min_min                      -0.001051\n",
       "min_positive_polarity           -0.000040\n",
       "n_non_stop_unique_tokens         0.000114\n",
       "n_non_stop_words                 0.000443\n",
       "global_rate_positive_words       0.000543\n",
       "n_unique_tokens                  0.000806\n",
       "abs_title_subjectivity           0.001481\n",
       "                                   ...   \n",
       "timedelta                        0.008662\n",
       "n_tokens_title                   0.008783\n",
       "weekday_is_monday                0.009726\n",
       "max_positive_polarity            0.010068\n",
       "avg_positive_polarity            0.012142\n",
       "title_sentiment_polarity         0.012772\n",
       "weekday_is_saturday              0.015082\n",
       "is_weekend                       0.016958\n",
       "num_keywords                     0.021818\n",
       "title_subjectivity               0.021967\n",
       "num_videos                       0.023936\n",
       "abs_title_sentiment_polarity     0.027135\n",
       "kw_max_min                       0.030114\n",
       "kw_avg_min                       0.030406\n",
       "global_subjectivity              0.031604\n",
       "readable_informative             0.039001\n",
       "num_imgs                         0.039388\n",
       "kw_min_avg                       0.039551\n",
       "readable_leisure                 0.040454\n",
       "readability_2                    0.041065\n",
       "kw_avg_max                       0.044686\n",
       "num_hrefs                        0.045404\n",
       "self_reference_max_shares        0.047115\n",
       "readability                      0.047196\n",
       "self_reference_min_shares        0.055958\n",
       "self_reference_avg_sharess       0.057789\n",
       "kw_max_avg                       0.064306\n",
       "LDA_03                           0.083771\n",
       "kw_avg_avg                       0.110413\n",
       "shares                           1.000000\n",
       "Name: shares, Length: 68, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_alt_corr['shares'].sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 - Modeling the Original df_OG\n",
    "Model your outcome of interest. You should try several different approaches and really work to tune a variety of models before using the model evaluation techniques to choose what you consider to be the best performer. Make sure to think about explanatory versus predictive power and experiment with both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared of the model in the training set is: 0.02047455701122769\n",
      "-----Test set statistics of OG-----\n",
      "R-squared of the model in the test set is: 0.047402848876784365\n",
      "Mean absolute error of the prediction is: 3010.594657108092\n",
      "Mean squared error of the prediction is: 62926848.052974775\n",
      "Root mean squared error of the prediction is: 7932.644455222658\n",
      "Mean absolute percentage error of the prediction is: 163.6679235011798\n",
      "\n",
      "\n",
      "\n",
      "R-squared of the model in the training set is: 0.020474557011227468\n",
      "-----Test set statistics of alt-----\n",
      "R-squared of the model in the test set is: 0.04740284887761426\n",
      "Mean absolute error of the prediction is: 3010.5946571051422\n",
      "Mean squared error of the prediction is: 62926848.052919954\n",
      "Root mean squared error of the prediction is: 7932.644455219202\n",
      "Mean absolute percentage error of the prediction is: 163.6679234980719\n"
     ]
    }
   ],
   "source": [
    "# import programs\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from statsmodels.tools.eval_measures import mse, rmse\n",
    "import numpy as np\n",
    "\n",
    "lrm = LinearRegression()\n",
    "lrm.fit(X_OG_train, y_OG_train)\n",
    "\n",
    "#X_OG_train, X_OG_test,\n",
    "#X_OG_train, X_OG_test, y_OG_train, y_OG_test\n",
    "\n",
    "# We are making predictions here\n",
    "y_OG_preds_train_OLS = lrm.predict(X_OG_train)\n",
    "y_OG_preds_test_OLS = lrm.predict(X_OG_test)\n",
    "\n",
    "print(\"R-squared of the model in the training set is: {}\".format(lrm.score(X_OG_train, y_OG_train)))\n",
    "print(\"-----Test set statistics of OG-----\")\n",
    "print(\"R-squared of the model in the test set is: {}\".format(lrm.score(X_OG_test, y_OG_test)))\n",
    "print(\"Mean absolute error of the prediction is: {}\".format(mean_absolute_error(y_OG_test, y_OG_preds_test_OLS)))\n",
    "print(\"Mean squared error of the prediction is: {}\".format(mse(y_OG_test, y_OG_preds_test_OLS)))\n",
    "print(\"Root mean squared error of the prediction is: {}\".format(rmse(y_OG_test, y_OG_preds_test_OLS)))\n",
    "print(\"Mean absolute percentage error of the prediction is: {}\".format(np.mean(np.abs((y_OG_test - y_OG_preds_test_OLS) / y_OG_test)) * 100))\n",
    "\n",
    "\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "#X_alt_train, X_alt_test, y_alt_train, y_alt_test \n",
    "\n",
    "lrm = LinearRegression()\n",
    "lrm.fit(X_alt_train, y_alt_train)\n",
    "\n",
    "# We are making predictions here\n",
    "y_alt_preds_train_OLS = lrm.predict(X_alt_train)\n",
    "y_alt_preds_test_OLS = lrm.predict(X_alt_test)\n",
    "\n",
    "print(\"R-squared of the model in the training set is: {}\".format(lrm.score(X_alt_train, y_alt_train)))\n",
    "print(\"-----Test set statistics of alt-----\")\n",
    "print(\"R-squared of the model in the test set is: {}\".format(lrm.score(X_alt_test, y_alt_test)))\n",
    "print(\"Mean absolute error of the prediction is: {}\".format(mean_absolute_error(y_alt_test, y_alt_preds_test_OLS)))\n",
    "print(\"Mean squared error of the prediction is: {}\".format(mse(y_alt_test, y_alt_preds_test_OLS)))\n",
    "print(\"Root mean squared error of the prediction is: {}\".format(rmse(y_alt_test, y_alt_preds_test_OLS)))\n",
    "print(\"Mean absolute percentage error of the prediction is: {}\".format(np.mean(np.abs((y_alt_test - y_alt_preds_test_OLS) / y_alt_test)) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2368094442953.412, tolerance: 483539011.9545006\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared of the model on the training set is: 0.020367889723159283\n",
      "-----Test set statistics of OG-----\n",
      "R-squared of the model on the test set is: 0.04762656658068676\n",
      "Mean absolute error of the prediction is: 3008.9701054437805\n",
      "Mean squared error of the prediction is: 62912069.6653388\n",
      "Root mean squared error of the prediction is: 7931.71290865591\n",
      "Mean absolute percentage error of the prediction is: 163.88064577978568\n",
      "\n",
      "\n",
      "\n",
      "R-squared of the model on the training set is: 0.020371870348879773\n",
      "-----Test set statistics of alt-----\n",
      "R-squared of the model on the test set is: 0.047638420894611344\n",
      "Mean absolute error of the prediction is: 3008.957000202916\n",
      "Mean squared error of the prediction is: 62911286.59076186\n",
      "Root mean squared error of the prediction is: 7931.663544979821\n",
      "Mean absolute percentage error of the prediction is: 163.8705636555034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2368431655775.164, tolerance: 483539011.9545006\n",
      "  positive)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lassoregr = Lasso(alpha=0.01) \n",
    "lassoregr.fit(X_OG_train, y_OG_train)\n",
    "\n",
    "# We are making predictions here\n",
    "y_OG_preds_train_lasso = lassoregr.predict(X_OG_train)\n",
    "y_OG_preds_test_lasso = lassoregr.predict(X_OG_test)\n",
    "\n",
    "print(\"R-squared of the model on the training set is: {}\".format(lassoregr.score(X_OG_train, y_OG_train)))\n",
    "print(\"-----Test set statistics of OG-----\")\n",
    "print(\"R-squared of the model on the test set is: {}\".format(lassoregr.score(X_OG_test, y_OG_test)))\n",
    "print(\"Mean absolute error of the prediction is: {}\".format(mean_absolute_error(y_OG_test, y_OG_preds_test_lasso)))\n",
    "print(\"Mean squared error of the prediction is: {}\".format(mse(y_OG_test, y_OG_preds_test_lasso)))\n",
    "print(\"Root mean squared error of the prediction is: {}\".format(rmse(y_OG_test, y_OG_preds_test_lasso)))\n",
    "print(\"Mean absolute percentage error of the prediction is: {}\".format(np.mean(np.abs((y_OG_test - y_OG_preds_test_lasso) / y_OG_test)) * 100))\n",
    "\n",
    "\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "\n",
    "\n",
    "#from sklearn.linear_model import Lasso\n",
    "\n",
    "#lassoregr = Lasso(alpha=0.01) \n",
    "lassoregr.fit(X_alt_train, y_alt_train)\n",
    "\n",
    "# We are making predictions here\n",
    "y_alt_preds_train_lasso = lassoregr.predict(X_alt_train)\n",
    "y_alt_preds_test_lasso = lassoregr.predict(X_alt_test)\n",
    "\n",
    "print(\"R-squared of the model on the training set is: {}\".format(lassoregr.score(X_alt_train, y_alt_train)))\n",
    "print(\"-----Test set statistics of alt-----\")\n",
    "print(\"R-squared of the model on the test set is: {}\".format(lassoregr.score(X_alt_test, y_alt_test)))\n",
    "print(\"Mean absolute error of the prediction is: {}\".format(mean_absolute_error(y_alt_test, y_alt_preds_test_lasso)))\n",
    "print(\"Mean squared error of the prediction is: {}\".format(mse(y_alt_test, y_alt_preds_test_lasso)))\n",
    "print(\"Root mean squared error of the prediction is: {}\".format(rmse(y_alt_test, y_alt_preds_test_lasso)))\n",
    "print(\"Mean absolute percentage error of the prediction is: {}\".format(np.mean(np.abs((y_alt_test - y_alt_preds_test_lasso) / y_alt_test)) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2369462727656.5835, tolerance: 483539011.9545006\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared of the model on the training set is: 0.02008534601176959\n",
      "-----Test set statistics of OG-----\n",
      "R-squared of the model on the test set is: 0.04770111328855364\n",
      "Mean absolute error of the prediction is: 3003.842697523595\n",
      "Mean squared error of the prediction is: 62907145.24439836\n",
      "Root mean squared error of the prediction is: 7931.402476510592\n",
      "Mean absolute percentage error of the prediction is: 164.54924449588967\n",
      "\n",
      "\n",
      "\n",
      "R-squared of the model on the training set is: 0.020097585734631385\n",
      "-----Test set statistics of alt-----\n",
      "R-squared of the model on the test set is: 0.04775115620009829\n",
      "Mean absolute error of the prediction is: 3003.8597438086563\n",
      "Mean squared error of the prediction is: 62903839.50000559\n",
      "Root mean squared error of the prediction is: 7931.194077817387\n",
      "Mean absolute percentage error of the prediction is: 164.47298897399787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2369499709472.2925, tolerance: 483539011.9545006\n",
      "  positive)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "elasticregr = ElasticNet(alpha=0.01, l1_ratio=0.5) \n",
    "elasticregr.fit(X_OG_train, y_OG_train)\n",
    "\n",
    "# We are making predictions here\n",
    "y_OG_preds_train_elasticnet = elasticregr.predict(X_OG_train)\n",
    "y_OG_preds_test_elasticnet = elasticregr.predict(X_OG_test)\n",
    "\n",
    "print(\"R-squared of the model on the training set is: {}\".format(elasticregr.score(X_OG_train, y_OG_train)))\n",
    "print(\"-----Test set statistics of OG-----\")\n",
    "print(\"R-squared of the model on the test set is: {}\".format(elasticregr.score(X_OG_test, y_OG_test)))\n",
    "print(\"Mean absolute error of the prediction is: {}\".format(mean_absolute_error(y_OG_test, y_OG_preds_test_elasticnet)))\n",
    "print(\"Mean squared error of the prediction is: {}\".format(mse(y_OG_test, y_OG_preds_test_elasticnet)))\n",
    "print(\"Root mean squared error of the prediction is: {}\".format(rmse(y_OG_test, y_OG_preds_test_elasticnet)))\n",
    "print(\"Mean absolute percentage error of the prediction is: {}\".format(np.mean(np.abs((y_OG_test - y_OG_preds_test_elasticnet) / y_OG_test)) * 100))\n",
    "\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "#from sklearn.linear_model import ElasticNet\n",
    "\n",
    "#elasticregr = ElasticNet(alpha=0.01, l1_ratio=0.5) \n",
    "elasticregr.fit(X_alt_train, y_alt_train)\n",
    "\n",
    "# We are making predictions here\n",
    "y_alt_preds_train_elasticnet = elasticregr.predict(X_alt_train)\n",
    "y_alt_preds_test_elasticnet = elasticregr.predict(X_alt_test)\n",
    "\n",
    "print(\"R-squared of the model on the training set is: {}\".format(elasticregr.score(X_alt_train, y_alt_train)))\n",
    "print(\"-----Test set statistics of alt-----\")\n",
    "print(\"R-squared of the model on the test set is: {}\".format(elasticregr.score(X_alt_test, y_alt_test)))\n",
    "print(\"Mean absolute error of the prediction is: {}\".format(mean_absolute_error(y_alt_test, y_alt_preds_test_elasticnet)))\n",
    "print(\"Mean squared error of the prediction is: {}\".format(mse(y_alt_test, y_alt_preds_test_elasticnet)))\n",
    "print(\"Root mean squared error of the prediction is: {}\".format(rmse(y_alt_test, y_alt_preds_test_elasticnet)))\n",
    "print(\"Mean absolute percentage error of the prediction is: {}\".format(np.mean(np.abs((y_alt_test - y_alt_preds_test_elasticnet) / y_alt_test)) * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared of the model on the training set is: 0.020474497371104072\n",
      "-----Test set statistics of OG-----\n",
      "R-squared of the model on the test set is: 0.047406713994894156\n",
      "Mean absolute error of the prediction is: 3010.6146934356575\n",
      "Mean squared error of the prediction is: 62926592.73025025\n",
      "Root mean squared error of the prediction is: 7932.628362040557\n",
      "Mean absolute percentage error of the prediction is: 163.66830997347373\n",
      "\n",
      "\n",
      "\n",
      "R-squared of the model on the training set is: 0.020474497374702638\n",
      "-----Test set statistics of alt-----\n",
      "R-squared of the model on the test set is: 0.04740672131714618\n",
      "Mean absolute error of the prediction is: 3010.6147625154845\n",
      "Mean squared error of the prediction is: 62926592.2465555\n",
      "Root mean squared error of the prediction is: 7932.628331552884\n",
      "Mean absolute percentage error of the prediction is: 163.6683237453969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=4.41442e-18): result may not be accurate.\n",
      "  overwrite_a=True).T\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=2.64634e-18): result may not be accurate.\n",
      "  overwrite_a=True).T\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Fitting a ridge regression model. Alpha is the regularization\n",
    "# parameter (usually called lambda). As alpha gets larger, parameter\n",
    "# shrinkage grows more pronounced.\n",
    "ridgeregr = Ridge(alpha=0.01) \n",
    "ridgeregr.fit(X_OG_train, y_OG_train)\n",
    "\n",
    "# We are making predictions here\n",
    "y_OG_preds_train_ridge = ridgeregr.predict(X_OG_train)\n",
    "y_OG_preds_test_ridge = ridgeregr.predict(X_OG_test)\n",
    "\n",
    "print(\"R-squared of the model on the training set is: {}\".format(ridgeregr.score(X_OG_train, y_OG_train)))\n",
    "print(\"-----Test set statistics of OG-----\")\n",
    "print(\"R-squared of the model on the test set is: {}\".format(ridgeregr.score(X_OG_test, y_OG_test)))\n",
    "print(\"Mean absolute error of the prediction is: {}\".format(mean_absolute_error(y_OG_test, y_OG_preds_test_ridge)))\n",
    "print(\"Mean squared error of the prediction is: {}\".format(mse(y_OG_test, y_OG_preds_test_ridge)))\n",
    "print(\"Root mean squared error of the prediction is: {}\".format(rmse(y_OG_test, y_OG_preds_test_ridge)))\n",
    "print(\"Mean absolute percentage error of the prediction is: {}\".format(np.mean(np.abs((y_OG_test - y_OG_preds_test_ridge) / y_OG_test)) * 100))\n",
    "\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "\n",
    "#from sklearn.linear_model import Ridge\n",
    "\n",
    "# Fitting a ridge regression model. Alpha is the regularization\n",
    "# parameter (usually called lambda). As alpha gets larger, parameter\n",
    "# shrinkage grows more pronounced.\n",
    "ridgeregr = Ridge(alpha=0.01) \n",
    "ridgeregr.fit(X_alt_train, y_alt_train)\n",
    "\n",
    "# We are making predictions here\n",
    "y_alt_preds_train_ridge = ridgeregr.predict(X_alt_train)\n",
    "y_alt_preds_test_ridge = ridgeregr.predict(X_alt_test)\n",
    "\n",
    "print(\"R-squared of the model on the training set is: {}\".format(ridgeregr.score(X_alt_train, y_alt_train)))\n",
    "print(\"-----Test set statistics of alt-----\")\n",
    "print(\"R-squared of the model on the test set is: {}\".format(ridgeregr.score(X_alt_test, y_alt_test)))\n",
    "print(\"Mean absolute error of the prediction is: {}\".format(mean_absolute_error(y_alt_test, y_alt_preds_test_ridge)))\n",
    "print(\"Mean squared error of the prediction is: {}\".format(mse(y_alt_test, y_alt_preds_test_ridge)))\n",
    "print(\"Root mean squared error of the prediction is: {}\".format(rmse(y_alt_test, y_alt_preds_test_ridge)))\n",
    "print(\"Mean absolute percentage error of the prediction is: {}\".format(np.mean(np.abs((y_alt_test - y_alt_preds_test_ridge) / y_alt_test)) * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the large errors in the models and looking at the insignificant correlation coefficients of the variables, we currently do not have sufficient information to determine which factors contribute to the number of shares an article gets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further Research on this article would be to look at the titles and seeing if they have 'buzz words.' I notice that on this dataframe, there are variables called rate of positive words, rate of negative words; but if there are isolated variables to include information on polarized words in the title, I believe this factor could predict article popularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
